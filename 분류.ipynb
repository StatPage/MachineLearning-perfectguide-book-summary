{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류\n",
    "\n",
    "분류는 지도학습의 대표적인 유형이다. 분류는 기존 데이터가 어떤 레이블에 속하는지 패턴을 알아낸 후에 새로운 데이터에 대해 레이블을 판별하는 것이다.\n",
    "\n",
    "분류의 종류: 나이브 베이즈, 로지스틱 회귀, 결정트리, 서포트 벡터 머신, 최소 근접 알고리즘, 신경망, 앙상블\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 결정트리\n",
    "\n",
    "데이터에 있는 규칙을 학습을 통해 찾아내 트리 기반의 분류 규칙을 만드는 것. 알고리즘의 성능은 데이터의 어떤 기준으로 규칙을 만들지에 결정이 된다.\n",
    "\n",
    "- 규칙 노드: 규칙 조건이 되는 노드\n",
    "- 리프 노드: 클래스가 결정된 노드\n",
    "\n",
    "결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. \n",
    "\n",
    "데이터 세트를 구분할 때 정보 균일도가 높게끔 조건을 찾아 서브 데이터 세트를 만들고 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정보 이득: 1 - 엔트로피 지수(데이터 집합의 혼잡도)\n",
    "- 지니 계수: 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등, 1이 가장 불평등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
      "       [4.9, 3. , 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.3, 0.2],\n",
      "       [4.6, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.6, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [5. , 3.4, 1.5, 0.2],\n",
      "       [4.4, 2.9, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.4, 3.7, 1.5, 0.2],\n",
      "       [4.8, 3.4, 1.6, 0.2],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5.1, 3.5, 1.4, 0.3],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [5.4, 3.4, 1.7, 0.2],\n",
      "       [5.1, 3.7, 1.5, 0.4],\n",
      "       [4.6, 3.6, 1. , 0.2],\n",
      "       [5.1, 3.3, 1.7, 0.5],\n",
      "       [4.8, 3.4, 1.9, 0.2],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [4.7, 3.2, 1.6, 0.2],\n",
      "       [4.8, 3.1, 1.6, 0.2],\n",
      "       [5.4, 3.4, 1.5, 0.4],\n",
      "       [5.2, 4.1, 1.5, 0.1],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.2],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [4.9, 3.6, 1.4, 0.1],\n",
      "       [4.4, 3. , 1.3, 0.2],\n",
      "       [5.1, 3.4, 1.5, 0.2],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [4.5, 2.3, 1.3, 0.3],\n",
      "       [4.4, 3.2, 1.3, 0.2],\n",
      "       [5. , 3.5, 1.6, 0.6],\n",
      "       [5.1, 3.8, 1.9, 0.4],\n",
      "       [4.8, 3. , 1.4, 0.3],\n",
      "       [5.1, 3.8, 1.6, 0.2],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [5. , 3.3, 1.4, 0.2],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [6.4, 3.2, 4.5, 1.5],\n",
      "       [6.9, 3.1, 4.9, 1.5],\n",
      "       [5.5, 2.3, 4. , 1.3],\n",
      "       [6.5, 2.8, 4.6, 1.5],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.3, 3.3, 4.7, 1.6],\n",
      "       [4.9, 2.4, 3.3, 1. ],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [5. , 2. , 3.5, 1. ],\n",
      "       [5.9, 3. , 4.2, 1.5],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.9, 4.7, 1.4],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [5.6, 3. , 4.5, 1.5],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [6.2, 2.2, 4.5, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1],\n",
      "       [5.9, 3.2, 4.8, 1.8],\n",
      "       [6.1, 2.8, 4. , 1.3],\n",
      "       [6.3, 2.5, 4.9, 1.5],\n",
      "       [6.1, 2.8, 4.7, 1.2],\n",
      "       [6.4, 2.9, 4.3, 1.3],\n",
      "       [6.6, 3. , 4.4, 1.4],\n",
      "       [6.8, 2.8, 4.8, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [6. , 2.9, 4.5, 1.5],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.5, 2.4, 3.8, 1.1],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [5.8, 2.7, 3.9, 1.2],\n",
      "       [6. , 2.7, 5.1, 1.6],\n",
      "       [5.4, 3. , 4.5, 1.5],\n",
      "       [6. , 3.4, 4.5, 1.6],\n",
      "       [6.7, 3.1, 4.7, 1.5],\n",
      "       [6.3, 2.3, 4.4, 1.3],\n",
      "       [5.6, 3. , 4.1, 1.3],\n",
      "       [5.5, 2.5, 4. , 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.6, 4. , 1.2],\n",
      "       [5. , 2.3, 3.3, 1. ],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5.7, 3. , 4.2, 1.2],\n",
      "       [5.7, 2.9, 4.2, 1.3],\n",
      "       [6.2, 2.9, 4.3, 1.3],\n",
      "       [5.1, 2.5, 3. , 1.1],\n",
      "       [5.7, 2.8, 4.1, 1.3],\n",
      "       [6.3, 3.3, 6. , 2.5],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [7.1, 3. , 5.9, 2.1],\n",
      "       [6.3, 2.9, 5.6, 1.8],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [4.9, 2.5, 4.5, 1.7],\n",
      "       [7.3, 2.9, 6.3, 1.8],\n",
      "       [6.7, 2.5, 5.8, 1.8],\n",
      "       [7.2, 3.6, 6.1, 2.5],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [6.4, 2.7, 5.3, 1.9],\n",
      "       [6.8, 3. , 5.5, 2.1],\n",
      "       [5.7, 2.5, 5. , 2. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.4, 3.2, 5.3, 2.3],\n",
      "       [6.5, 3. , 5.5, 1.8],\n",
      "       [7.7, 3.8, 6.7, 2.2],\n",
      "       [7.7, 2.6, 6.9, 2.3],\n",
      "       [6. , 2.2, 5. , 1.5],\n",
      "       [6.9, 3.2, 5.7, 2.3],\n",
      "       [5.6, 2.8, 4.9, 2. ],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [6.3, 2.7, 4.9, 1.8],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.2, 3.2, 6. , 1.8],\n",
      "       [6.2, 2.8, 4.8, 1.8],\n",
      "       [6.1, 3. , 4.9, 1.8],\n",
      "       [6.4, 2.8, 5.6, 2.1],\n",
      "       [7.2, 3. , 5.8, 1.6],\n",
      "       [7.4, 2.8, 6.1, 1.9],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.4, 2.8, 5.6, 2.2],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [6.3, 3.4, 5.6, 2.4],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [6. , 3. , 4.8, 1.8],\n",
      "       [6.9, 3.1, 5.4, 2.1],\n",
      "       [6.7, 3.1, 5.6, 2.4],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [5.8, 2.7, 5.1, 1.9],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [6.7, 3.3, 5.7, 2.5],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [6.2, 3.4, 5.4, 2.3],\n",
      "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'C:\\\\Users\\\\K\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)  # 원하면 min_samples_leaf나 max_depth등의 조건을 추가할 수 있다.\n",
    "iris_data = load_iris()  # 역시 iris 데이터가 최고다.\n",
    "print(iris_data)  # iris_data 내에 data와 target이 따로있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, random_state=0)  # 원하면 test_size = 를 통해 데이터의 비율을 정할 수 있다. radom_state도 설정 가능\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 수치: 0.9640316205533598\n",
      "최적의 하이퍼 파라미터: {'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "### GridSearchCV를 사용한 학습과 예측\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'max_depth' : [0,2,4,6,8,10,12,16,20,24]}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5)  # 예측 성능을 측정할 평가 방법으로 scoring인자를 입력한다. 보통 문자열('accuracy')을 입력한다.\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('정확도 수치:', grid_cv.best_score_)\n",
    "print('최적의 하이퍼 파라미터:', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도가 GridSearch를 사용했음에도 낮아졌지만 애초에 정확도가 높았기에 사용할 필요는 없었다. GridSearch는 정확도가 낮아 정확도를 높이기 위해 최적의 하이퍼 파라미터를 찾을 필요가 생기면 수행하는 게 낫다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV(교차 검증)이란?\n",
    " GridSearch(교차검증)은 데이터 세트를 cross-validation을 위한 데이터 세트를 자동으로 나누어 하이퍼 파라미터 그리드에 기술된 모든 파라미터를 순차적으로 적용해 최적의 파라미터를 찾을 수 있게 해줍니다.\n",
    " \n",
    "  교차 검증은 데이터의 편중을 막기 위해 여러 여러 개의 학습 데이터 세트와 검증 데이터 세트로 학습과 평가를 수행한다. 각 세트에서 수행한 결과를 바탕으로 모델 최적화를 할 수 있다.\n",
    "  \n",
    "- K폴드 교차 검증: K개의 데이터 폴드 세트를 만들어서 학습과 평가를 K번 수행하는 방법이다.\n",
    "- Stratified K폴드: 데이터가 불균형한 분포도를 가지고 있을 때 사용하는 K폴드 방식.\n",
    "\n",
    "GridSearchCV의 가장 큰 특징은 인자 중에서 하이퍼 파라미터 그리드(param_grid)에 입력된 모든 파라미터를 순차적으로 적용해 최적의 파라미터를 찾게 해주는 것이다.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 학습\n",
    "여러 개의 분류기를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법을 말합니다.\n",
    "- 보팅: 다른 알고리즘을 가진 분류기로 같은 데이터 세트에 대해 학습하고 예측한 결과를 가지고 보팅을 통해 최종 예측 결과를 선정하는 것.\n",
    "- 배깅: 모두 같은 유형의 알고리즘 기반이지만 데이터 샘플링을 서로 다르게 가져가서(부트스트래핑 분할 방식) 학습을 수행해 보팅을 수행하는 것.\n",
    "- 부스팅: 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 부여하면서 학습과 예측을 진행하는 것. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 소프트 보팅: 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정.\n",
    "2. 하드 보팅: 예측한 결괏값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정.\n",
    "\n",
    "__보통 소프트 보팅을 사용한다.__\n",
    "\n",
    "### 2. 소프트 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# 개별 모델은 로지스틱 회귀와 KNN(최근접 이웃알고리즘)이다. 이것들은 뒤에서 다루자!!\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "data_df.head(3)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "vo_clf = VotingClassifier(estimators=[('LR',lr_clf), ('KNN', knn_clf)], voting='soft')\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print(pred)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 랜덤 포레스트\n",
    "랜덤 포레스트는 배깅의 대표적인 알고리즘 방식이다. 배깅은 보팅과 다르게, 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정을 한다.\n",
    "\n",
    "특징: \n",
    "- 앙상블 알고리즘 중에서 빠른 수행 속도.\n",
    "- 다양한 영역에서 높은 예측 성능.\n",
    "- 결정 트리의 쉽고 직관적이다.\n",
    "\n",
    "여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 소프트 보팅을 통해 예측 결정. 데이터 세트는 여러 개를 중첩되게 분리하는 것을 부트스트래핑."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=2020)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "print(pred)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "트리 기반의 앙상블 알고리즘의 단점(굳이 단점이라고 하자면)은 하이퍼 파라미터가 너무 많고 튜닝을 위한 시간이 많이 소모되는 것. 그래서 GridSearchCV를 이용해 랜덤포레스트를 튜닝 시간을 절약할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9389671361502347\n",
      "{'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 100}\n",
      "0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "# 책에 있는 하이퍼 파라미터를 쓰기 위해 데이터를 불러오고 GridSearch 수행.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=2020)\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[100],\n",
    "    'max_depth':[6,8,10,12],       \n",
    "    'min_samples_leaf':[8,12,18],  #말단 노드(leaf)가 되기 위한 최소한의 샘플 데이터 수 \n",
    "    'min_samples_split':[8,16,20]  #노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용.\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=2020, n_jobs = -1)    \n",
    "grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs= -1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print(grid_cv.best_score_)\n",
    "print(grid_cv.best_params_)\n",
    "\n",
    "rf_clf1 = RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_leaf=8, min_samples_split=8, random_state=2020)\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "pred=rf_clf1.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적 하이퍼 파라미터를 GridSearchCV를 통해 찾을 수 있었고 랜덤포레스트에 적용해 예측한 결괏값의 정확도가 더욱 올라간 것을 알 수 있다.\n",
    "\n",
    "랜덤 포레스트는 CPU 병렬 처리도 효과적으로 수행되어 빠른 학습이 가능하다. 멀티 코어 환경에서는 RandomClassifier 생성자와 GridSearchCV 생성시 n_jobs를 추가하면 모든 CPU 코어를 이용해 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GBM(Gradient Boosting Machine)\n",
    "부스팅 알고리즘을 구현한 대표적인 방법\n",
    "- 부스팅 알고리즘: 여러 개의 약한 학습기를 순차적으로 학습 예측하면서 잘못 예측한 데이터에 가중치를 부여하는 알고리즘. 대표적으로는 AdaBoost와 GradientBoost가 있다.\n",
    "\n",
    "GBM은 가중치 업데이트를 경사 하강법을 이용한다.  \n",
    "\n",
    "- 경사 하강법: 분류의 실제 결괏값을 y, 피처를 $x_1,x_2,...,x_n$, 그리고 이 피처에 기반한 예측 함수를 $F(x)$함수라고 하면 오류식 $h(x) = y - F(x)$가 된다. 이 오류식 $h(x)$를 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것.  \n",
    "\n",
    "보통 랜덤 포레스트보다 예측이 뛰어난 경우가 많으나 수행 시간 문제가 존재한다. 그 이유는 사이킷런의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측, 오류 보정을 통해 학습을 수행해서 병렬 처리가 지원이 안돼므로 학습할 시간이 많이 필요하기 때문이다.  \n",
    "경사 하강법에 대해서 책은 간단하게 '반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법' 정도로 이해해도 된다고 한다. 다음 장에서 배울테니..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965034965034965\n",
      "0.5566556453704834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "start_time = time.time()\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(gb_accuracy)\n",
    "print(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. XGBoost\n",
    "Gradient Boosting 알고리즘을 분산환경에서도 실행할 수 있도록 구현해놓은 라이브러리.  \n",
    "- 장점:뛰어난 예측 성능, GBM 대비 빠른 수행시간, 과적합 규제, Tree pruning, 자체 내장된 교차 검증, 결손값 자체 처리   \n",
    "\n",
    "파이썬 기반의 머신러닝 이용자들이 사이킷런을 많이 사용하고 있기 때문에 사이킷런과 연동되는 모듈을 제공. 사이킷런래퍼 XGBoost모듈이라고 할 수 있겠다.  \n",
    "XGBoost는 자체적으로 교차 검증, 성능 평가, 피처 중요도 등의 시각화기능을 가지고 있다. 수행 속도를 향상시키기 위해서 조기 중단 기능 또한 가지고 있는데 n_estimators에 지정한 반복 횟수에 도달하지 않아도 예측 오류가 개선되지 않으면 중지해서 수행 시간을 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " conda install -c anaconda py-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost를 설치하기 위해서는 아나콘다 프롬프트를 관리자 권한으로 실행한 수 위의 명령어를 수행해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90\n"
     ]
    }
   ],
   "source": [
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30)\n",
      "오차행렬\n",
      " [[35  2]\n",
      " [ 1 76]]\n",
      "정확도: 0.9736842105263158\n",
      "정밀도: 0.9743589743589743\n",
      "재현율: 0.987012987012987\n",
      "f1: 0.9806451612903225\n",
      "AUC: 0.995085995085995\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cancer=load_breast_cancer()\n",
    "X_features=cancer.data\n",
    "y_label = cancer.target\n",
    "cancer_df=pd.DataFrame(data=X_features, columns=cancer.feature_names)\n",
    "cancer_df['target']=y_label\n",
    "cancer_df.head(3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "params = { 'max_depth':3,\n",
    "           'eta': 0.1,\n",
    "           'objective':'binary:logistic',\n",
    "           'eval_metric':'logloss'\n",
    "        }\n",
    "num_rounds = 400\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "xgb_wrapper.fit(X_train, y_train)\n",
    "pred = xgb_wrapper.predict(X_test)\n",
    "pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "print(\"오차행렬\\n\", confusion_matrix(y_test, pred))\n",
    "print(\"정확도:\", accuracy_score(y_test , pred))\n",
    "print(\"정밀도:\", precision_score(y_test , pred))\n",
    "print(\"재현율:\", recall_score(y_test , pred))\n",
    "print(\"f1:\", f1_score(y_test,pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. LightGBM\n",
    "리프 중심 트리 분할 방식을 사용한 GBM.\n",
    "- 원리: 트리의 균형을 맞추지 않고(상대적으로 과적합에 강한 구조를 포기하고) 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하는 방법. 트리의 깊이가 깊어지고 비대칭적인 트리가 생성되나 이를 통해 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다.\n",
    "\n",
    "XGBoost에서 GridSearchCV로 하이퍼 파라미터 튜닝을 수행하면 수행 시간이 오래 걸리는 단점을 보완해준다. LightGBM은 메모리 샤용량도 상대적으로 적으나 둘의 예측 성능 차이는 얼마 없다고 한다.  \n",
    "유일한 단점으로는 과적합이 발생할 수도 있는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge lightgbm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM을 설치하기 위해서는 아나콘다 프롬프트를 관리자 권한으로 실행한 수 위의 명령어를 수행해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.565079\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.507451\n",
      "[3]\tvalid_0's binary_logloss: 0.458489\n",
      "[4]\tvalid_0's binary_logloss: 0.417481\n",
      "[5]\tvalid_0's binary_logloss: 0.385507\n",
      "[6]\tvalid_0's binary_logloss: 0.355773\n",
      "[7]\tvalid_0's binary_logloss: 0.329587\n",
      "[8]\tvalid_0's binary_logloss: 0.308478\n",
      "[9]\tvalid_0's binary_logloss: 0.285395\n",
      "[10]\tvalid_0's binary_logloss: 0.267055\n",
      "[11]\tvalid_0's binary_logloss: 0.252013\n",
      "[12]\tvalid_0's binary_logloss: 0.237018\n",
      "[13]\tvalid_0's binary_logloss: 0.224756\n",
      "[14]\tvalid_0's binary_logloss: 0.213383\n",
      "[15]\tvalid_0's binary_logloss: 0.203058\n",
      "[16]\tvalid_0's binary_logloss: 0.194015\n",
      "[17]\tvalid_0's binary_logloss: 0.186412\n",
      "[18]\tvalid_0's binary_logloss: 0.179108\n",
      "[19]\tvalid_0's binary_logloss: 0.174004\n",
      "[20]\tvalid_0's binary_logloss: 0.167155\n",
      "[21]\tvalid_0's binary_logloss: 0.162494\n",
      "[22]\tvalid_0's binary_logloss: 0.156886\n",
      "[23]\tvalid_0's binary_logloss: 0.152855\n",
      "[24]\tvalid_0's binary_logloss: 0.151113\n",
      "[25]\tvalid_0's binary_logloss: 0.148395\n",
      "[26]\tvalid_0's binary_logloss: 0.145869\n",
      "[27]\tvalid_0's binary_logloss: 0.143036\n",
      "[28]\tvalid_0's binary_logloss: 0.14033\n",
      "[29]\tvalid_0's binary_logloss: 0.139609\n",
      "[30]\tvalid_0's binary_logloss: 0.136109\n",
      "[31]\tvalid_0's binary_logloss: 0.134867\n",
      "[32]\tvalid_0's binary_logloss: 0.134729\n",
      "[33]\tvalid_0's binary_logloss: 0.1311\n",
      "[34]\tvalid_0's binary_logloss: 0.131143\n",
      "[35]\tvalid_0's binary_logloss: 0.129435\n",
      "[36]\tvalid_0's binary_logloss: 0.128474\n",
      "[37]\tvalid_0's binary_logloss: 0.126683\n",
      "[38]\tvalid_0's binary_logloss: 0.126112\n",
      "[39]\tvalid_0's binary_logloss: 0.122831\n",
      "[40]\tvalid_0's binary_logloss: 0.123162\n",
      "[41]\tvalid_0's binary_logloss: 0.125592\n",
      "[42]\tvalid_0's binary_logloss: 0.128293\n",
      "[43]\tvalid_0's binary_logloss: 0.128123\n",
      "[44]\tvalid_0's binary_logloss: 0.12789\n",
      "[45]\tvalid_0's binary_logloss: 0.122818\n",
      "[46]\tvalid_0's binary_logloss: 0.12496\n",
      "[47]\tvalid_0's binary_logloss: 0.125578\n",
      "[48]\tvalid_0's binary_logloss: 0.127381\n",
      "[49]\tvalid_0's binary_logloss: 0.128349\n",
      "[50]\tvalid_0's binary_logloss: 0.127004\n",
      "[51]\tvalid_0's binary_logloss: 0.130288\n",
      "[52]\tvalid_0's binary_logloss: 0.131362\n",
      "[53]\tvalid_0's binary_logloss: 0.133363\n",
      "[54]\tvalid_0's binary_logloss: 0.1332\n",
      "[55]\tvalid_0's binary_logloss: 0.134543\n",
      "[56]\tvalid_0's binary_logloss: 0.130803\n",
      "[57]\tvalid_0's binary_logloss: 0.130306\n",
      "[58]\tvalid_0's binary_logloss: 0.132514\n",
      "[59]\tvalid_0's binary_logloss: 0.133278\n",
      "[60]\tvalid_0's binary_logloss: 0.134804\n",
      "[61]\tvalid_0's binary_logloss: 0.136888\n",
      "[62]\tvalid_0's binary_logloss: 0.138745\n",
      "[63]\tvalid_0's binary_logloss: 0.140497\n",
      "[64]\tvalid_0's binary_logloss: 0.141368\n",
      "[65]\tvalid_0's binary_logloss: 0.140764\n",
      "[66]\tvalid_0's binary_logloss: 0.14348\n",
      "[67]\tvalid_0's binary_logloss: 0.143418\n",
      "[68]\tvalid_0's binary_logloss: 0.143682\n",
      "[69]\tvalid_0's binary_logloss: 0.145076\n",
      "[70]\tvalid_0's binary_logloss: 0.14686\n",
      "[71]\tvalid_0's binary_logloss: 0.148051\n",
      "[72]\tvalid_0's binary_logloss: 0.147664\n",
      "[73]\tvalid_0's binary_logloss: 0.149478\n",
      "[74]\tvalid_0's binary_logloss: 0.14708\n",
      "[75]\tvalid_0's binary_logloss: 0.14545\n",
      "[76]\tvalid_0's binary_logloss: 0.148767\n",
      "[77]\tvalid_0's binary_logloss: 0.149959\n",
      "[78]\tvalid_0's binary_logloss: 0.146083\n",
      "[79]\tvalid_0's binary_logloss: 0.14638\n",
      "[80]\tvalid_0's binary_logloss: 0.148461\n",
      "[81]\tvalid_0's binary_logloss: 0.15091\n",
      "[82]\tvalid_0's binary_logloss: 0.153011\n",
      "[83]\tvalid_0's binary_logloss: 0.154807\n",
      "[84]\tvalid_0's binary_logloss: 0.156501\n",
      "[85]\tvalid_0's binary_logloss: 0.158586\n",
      "[86]\tvalid_0's binary_logloss: 0.159819\n",
      "[87]\tvalid_0's binary_logloss: 0.161745\n",
      "[88]\tvalid_0's binary_logloss: 0.162829\n",
      "[89]\tvalid_0's binary_logloss: 0.159142\n",
      "[90]\tvalid_0's binary_logloss: 0.156765\n",
      "[91]\tvalid_0's binary_logloss: 0.158625\n",
      "[92]\tvalid_0's binary_logloss: 0.156832\n",
      "[93]\tvalid_0's binary_logloss: 0.154616\n",
      "[94]\tvalid_0's binary_logloss: 0.154263\n",
      "[95]\tvalid_0's binary_logloss: 0.157156\n",
      "[96]\tvalid_0's binary_logloss: 0.158617\n",
      "[97]\tvalid_0's binary_logloss: 0.157495\n",
      "[98]\tvalid_0's binary_logloss: 0.159413\n",
      "[99]\tvalid_0's binary_logloss: 0.15847\n",
      "[100]\tvalid_0's binary_logloss: 0.160746\n",
      "[101]\tvalid_0's binary_logloss: 0.16217\n",
      "[102]\tvalid_0's binary_logloss: 0.165293\n",
      "[103]\tvalid_0's binary_logloss: 0.164749\n",
      "[104]\tvalid_0's binary_logloss: 0.167097\n",
      "[105]\tvalid_0's binary_logloss: 0.167697\n",
      "[106]\tvalid_0's binary_logloss: 0.169462\n",
      "[107]\tvalid_0's binary_logloss: 0.169947\n",
      "[108]\tvalid_0's binary_logloss: 0.171\n",
      "[109]\tvalid_0's binary_logloss: 0.16907\n",
      "[110]\tvalid_0's binary_logloss: 0.169521\n",
      "[111]\tvalid_0's binary_logloss: 0.167719\n",
      "[112]\tvalid_0's binary_logloss: 0.166648\n",
      "[113]\tvalid_0's binary_logloss: 0.169053\n",
      "[114]\tvalid_0's binary_logloss: 0.169613\n",
      "[115]\tvalid_0's binary_logloss: 0.170059\n",
      "[116]\tvalid_0's binary_logloss: 0.1723\n",
      "[117]\tvalid_0's binary_logloss: 0.174733\n",
      "[118]\tvalid_0's binary_logloss: 0.173526\n",
      "[119]\tvalid_0's binary_logloss: 0.1751\n",
      "[120]\tvalid_0's binary_logloss: 0.178254\n",
      "[121]\tvalid_0's binary_logloss: 0.182968\n",
      "[122]\tvalid_0's binary_logloss: 0.179017\n",
      "[123]\tvalid_0's binary_logloss: 0.178326\n",
      "[124]\tvalid_0's binary_logloss: 0.177149\n",
      "[125]\tvalid_0's binary_logloss: 0.179171\n",
      "[126]\tvalid_0's binary_logloss: 0.180948\n",
      "[127]\tvalid_0's binary_logloss: 0.183861\n",
      "[128]\tvalid_0's binary_logloss: 0.187579\n",
      "[129]\tvalid_0's binary_logloss: 0.188122\n",
      "[130]\tvalid_0's binary_logloss: 0.1857\n",
      "[131]\tvalid_0's binary_logloss: 0.187442\n",
      "[132]\tvalid_0's binary_logloss: 0.188578\n",
      "[133]\tvalid_0's binary_logloss: 0.189729\n",
      "[134]\tvalid_0's binary_logloss: 0.187313\n",
      "[135]\tvalid_0's binary_logloss: 0.189279\n",
      "[136]\tvalid_0's binary_logloss: 0.191068\n",
      "[137]\tvalid_0's binary_logloss: 0.192414\n",
      "[138]\tvalid_0's binary_logloss: 0.191255\n",
      "[139]\tvalid_0's binary_logloss: 0.193453\n",
      "[140]\tvalid_0's binary_logloss: 0.196969\n",
      "[141]\tvalid_0's binary_logloss: 0.196378\n",
      "[142]\tvalid_0's binary_logloss: 0.196367\n",
      "[143]\tvalid_0's binary_logloss: 0.19869\n",
      "[144]\tvalid_0's binary_logloss: 0.200352\n",
      "[145]\tvalid_0's binary_logloss: 0.19712\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's binary_logloss: 0.122818\n",
      "오차행렬\n",
      " [[33  4]\n",
      " [ 1 76]]\n",
      "정확도: 0.956140350877193\n",
      "정밀도: 0.95\n",
      "재현율: 0.987012987012987\n",
      "f1: 0.9681528662420381\n",
      "AUC: 0.9905229905229904\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset=load_breast_cancer()\n",
    "ftr = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ftr, target, test_size=0.2, random_state=156)\n",
    "\n",
    "lgbm_wrapper = LGBMClassifier(n_estimators=400)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "pred = lgbm_wrapper.predict(X_test)\n",
    "pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"오차행렬\\n\", confusion_matrix(y_test, pred))\n",
    "print(\"정확도:\", accuracy_score(y_test , pred))\n",
    "print(\"정밀도:\", precision_score(y_test , pred))\n",
    "print(\"재현율:\", recall_score(y_test , pred))\n",
    "print(\"f1:\", f1_score(y_test,pred))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
